# Streaming Platforms DLT Pipeline

## ğŸ“ Project Structure

```
streaming_platforms_dlt/
â”œâ”€â”€ dag_loader.py                    # ğŸš€ Main DAG entry point (loaded by Airflow)
â”œâ”€â”€ streaming_platforms_dlt.yaml    # âš™ï¸  DAG configuration (tasks, dependencies)
â”œâ”€â”€ functions.py                     # ğŸ”§ All pipeline functions (core logic)
â”œâ”€â”€ test_pipeline.py                # ğŸ§ª Local testing script
â”œâ”€â”€ README.md                       # ğŸ“– Documentation
â””â”€â”€ PROJECT_STRUCTURE.md            # ğŸ“‹ This file
```

## ğŸ”§ Core Files

### **`dag_loader.py`**
- **Purpose**: Airflow DAG entry point
- **Used by**: Airflow scheduler
- **Contains**: DAG factory loader code

### **`streaming_platforms_dlt.yaml`**
- **Purpose**: DAG configuration
- **Used by**: `dag_loader.py`
- **Contains**: Task definitions, dependencies, schedules

### **`functions.py`**
- **Purpose**: All pipeline logic
- **Used by**: Airflow tasks (via YAML config)
- **Contains**:
  - `load_csv_to_dlt()` - Load CSV files to DuckDB
  - `create_unified_table()` - Merge data by title
  - `validate_data()` - Data validation and stats
  - `analyze_cross_platform_rankings()` - Cross-platform analysis

## ğŸ§ª Testing

### **`test_pipeline.py`**
- **Purpose**: Local testing outside Airflow
- **Usage**:
  ```bash
  # Full pipeline test
  python test_pipeline.py

  # Quick validation only
  python test_pipeline.py --quick

  # Clean test databases
  python test_pipeline.py --clean
  ```

## ğŸ¯ Best Practices

### âœ… **What We Do Right**
1. **Single source of truth**: All functions in `functions.py`
2. **Configuration-driven**: DAG defined in YAML
3. **Testable**: Separate test script for local development
4. **Clean separation**: DAG loading vs. business logic

### ğŸš€ **Why This Structure Works**
1. **Maintainable**: One file per concern
2. **Testable**: Can test functions independently
3. **Scalable**: Easy to add new functions to `functions.py`
4. **Airflow-friendly**: Uses DAG Factory pattern

## ğŸ”„ Development Workflow

1. **Add new function** â†’ Edit `functions.py`
2. **Add new task** â†’ Edit `streaming_platforms_dlt.yaml`
3. **Test locally** â†’ Run `python test_pipeline.py`
4. **Deploy** â†’ Airflow picks up changes automatically

## ğŸ“Š Data Flow

```
CSV Files â†’ load_csv_to_dlt() â†’ Individual DuckDB files
                                       â†“
Unified Table â† create_unified_table() â† Platform DBs
     â†“
[validate_data() + analyze_cross_platform_rankings()]
     â†“
Final Reports & Statistics
```
